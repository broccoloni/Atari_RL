{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645a43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ale_py as a\n",
    "import time\n",
    "import numpy as np\n",
    "from AtariNet import *\n",
    "from helper_functions import *\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f696295",
   "metadata": {},
   "source": [
    "Uncomment and run the code below to select a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd3f20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [print(i,j) for i,j in enumerate(gym.envs.registry)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba53a5b",
   "metadata": {},
   "source": [
    "# Genetic Algorithm\n",
    "\n",
    "If training with a Genetic algorithm, select the parameters below, if not, continue to policy gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968d8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to change for each run\n",
    "\n",
    "#select a game from the above and set game as a string of the game name\n",
    "game = \"DemonAttack-v5\"\n",
    "\n",
    "#set the number of generations to train\n",
    "num_gens = 10\n",
    "\n",
    "#The current number of generations trained\n",
    "cur_gen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "published-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment setup\n",
    "ale = a.ALEInterface()\n",
    "env = gym.make(f'ALE/{game}')\n",
    "env = gym.wrappers.GrayScaleObservation(env)\n",
    "\n",
    "#hyperparameters\n",
    "numactions = env.action_space.n                  #The number of possible actions a player can make\n",
    "inshape = [1]+list(env.observation_space.shape)  #The shape of the observation/screen of the game\n",
    "mut_power = 0.002                                #Mutation power in genetic algorithm\n",
    "pop_size = 500                                   #The size of the genetic algorithm population\n",
    "arch_size = 10                                   #The size of the archive (those that stay without mutation)\n",
    "test_size = 10                                   #The size of the test set when evaluating fitness\n",
    "numsurvivors = 50                                #The number of survivors from each population\n",
    "poolsizes = [2,2]                                #The pooling in each dimension\n",
    "numconvlayers = 3                                #Number of convolutional layers in the network\n",
    "maxt = 1000                                      #max number of frames\n",
    "almightyint = 1234                               #used for seeding\n",
    "\n",
    "#for convenience when calling functions\n",
    "params = [game,          #0\n",
    "          cur_gen,       #1\n",
    "          numactions,    #2\n",
    "          inshape,       #3\n",
    "          mut_power,     #4\n",
    "          pop_size,      #5\n",
    "          arch_size,     #6\n",
    "          test_size,     #7\n",
    "          numsurvivors,  #8\n",
    "          poolsizes,     #9\n",
    "          numconvlayers, #10\n",
    "          maxt,          #11\n",
    "          almightyint]   #12\n",
    "\n",
    "\n",
    "#Create population, and mutations\n",
    "if cur_gen != 0:\n",
    "    population,mutations,avg_rewards = load_gen(game,cur_gen)\n",
    "    select_and_mutate(population,mutations,avg_rewards,pop_size,trunc)\n",
    "\n",
    "else:\n",
    "    population = np.random.randint(0,almightyint,size = pop_size+arch_size)\n",
    "    mutations = [[] for i in range(pop_size+arch_size)]\n",
    "    avg_rewards = np.zeros(arch_size+pop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the agents\n",
    "train_GA(num_gens,params,population,mutations,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32e514",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "We'll use an actor-critic method to train an agent to play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c139d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Things to change for each run\n",
    "\n",
    "#select a game from the above and set game as a string of the game name\n",
    "game = \"DemonAttack-v5\"\n",
    "\n",
    "#Current number of batches trained\n",
    "cur_batches = 34700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9fccb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment setup\n",
    "ale = a.ALEInterface()\n",
    "env = gym.make(f'ALE/{game}')\n",
    "env = gym.wrappers.GrayScaleObservation(env)\n",
    "\n",
    "#hyperparameters\n",
    "gamma = 0.99                    #decay of memory of previous runs\n",
    "atarilr = 0.001                 #learning rate for actor\n",
    "criticlr = 0.001                #learning rate for critic\n",
    "num_batches = 5001              #number of batches for training\n",
    "batch_size = 1                  #number of runs in each batch\n",
    "numactions = env.action_space.n #number of actions an agent can take\n",
    "maxt = 3000                     #max number of frames\n",
    "inshape = list(env.observation_space.shape) #[210,160]\n",
    "midsize = 100                   #size of the middle fully connected layer\n",
    "\n",
    "params = [game,          #0\n",
    "          cur_batches,   #1\n",
    "          num_batches,   #2\n",
    "          batch_size,    #3\n",
    "          maxt,          #4\n",
    "          gamma,         #5\n",
    "          midsize,       #6 \n",
    "          numactions]    #7\n",
    "\n",
    "#initialize models\n",
    "actor = AtariNetFC(inshape = inshape, midsize = midsize, outsize = numactions)\n",
    "critic = AtariNetFC(inshape = inshape, midsize = midsize, outsize = 1)\n",
    "actorOptimizer = torch.optim.Adam(actor.parameters(), lr = atarilr, weight_decay = 0)\n",
    "criticOptimizer = torch.optim.Adam(critic.parameters(), lr = criticlr, weight_decay = 0)\n",
    "\n",
    "#keep track of rewards, estimates, and batch lengths\n",
    "sum_rewards = []\n",
    "sum_rewards_est = []\n",
    "batch_lens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5505a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load previous runs\n",
    "if cur_batches != 0:\n",
    "    measures, actorOptimizer = actor.load(\"PGsaves/{}_m{}_{}.tar\".format(game,midsize,cur_batches),actorOptimizer)\n",
    "    measures, criticOptimizer = critic.load(\"PGsaves/Critic_{}_m{}_{}.tar\".format(game,midsize,cur_batches),criticOptimizer)\n",
    "\n",
    "    sum_rewards = measures[0]\n",
    "    sum_rewards_est = measures[1]\n",
    "    episode_lens = measures[2]\n",
    "    cur_batches = len(sum_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ea3eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 5001/5001 [12:48<00:00,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train the agent\n",
    "train_PG(params,\n",
    "         sum_rewards,\n",
    "         sum_rewards_est,\n",
    "         batch_lens,\n",
    "         env,actor,\n",
    "         actorOptimizer,\n",
    "         critic,\n",
    "         criticOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cc21295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1001 timesteps\n"
     ]
    }
   ],
   "source": [
    "#increase maxt\n",
    "params[4] = 10000\n",
    "\n",
    "#make gif\n",
    "make_gameplay_gif(params,actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc171482",
   "metadata": {},
   "source": [
    "Run this line to play the video of the gameplay\n",
    "\n",
    "Note: jupyter saves checkpoints and often this will cause the gif to not update when rerunning this.\n",
    "To account for this, change the number after the question mark in this markdown and it will load the\n",
    "newest version\n",
    "\n",
    "<img src=\"./gameplay.gif?5\" width=\"250\" height=\"250\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37c4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
